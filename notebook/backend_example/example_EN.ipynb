{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9567b36",
   "metadata": {},
   "source": [
    "# PYNQTorch Project Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bda187",
   "metadata": {},
   "source": [
    "First, import dependencies and our custom operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aef69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "from pytorch_zynq import init_hardware as init, mmult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28365203",
   "metadata": {},
   "source": [
    "The basis of **AI acceleration** is matrix multiplication. Registering the `PrivateUseOne` interface in the torch backend and renaming it to `zynq` allows us to easily register the functions we want. We need to declare the device's functions and behaviors in the operator. Below is the declaration of the registered device:\n",
    "```python\n",
    "from .device import register_zynq_device, is_registered\n",
    "from .device import enable_full_device, disable_full_device\n",
    "from .device import enable_implicit_accel, disable_implicit_accel\n",
    "from .ops import mmult, register_aten_impls\n",
    "from .linear import ZynqLinear\n",
    "from .hardware import init as init_hardware, is_hardware_available, deinit as deinit_hardware\n",
    "\n",
    "__all__ = [\n",
    "    \"register_zynq_device\",\n",
    "    \"is_registered\",\n",
    "    \"mmult\",\n",
    "    \"ZynqLinear\",\n",
    "    \"init_hardware\",\n",
    "    \"is_hardware_available\",\n",
    "    \"register_aten_impls\",\n",
    "    \"deinit_hardware\",\n",
    "    \"enable_full_device\",\n",
    "    \"disable_full_device\",\n",
    "    \"enable_implicit_accel\",\n",
    "    \"disable_implicit_accel\",\n",
    "    \n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9c2c8",
   "metadata": {},
   "source": [
    "The code above shows the behaviors and acceleration operations supported by our dual-pipeline Matrix Multiplication (**GeMM**) operator. How do we bind tensors to the operator? In fact, our hardware does not support loading tensors directly. However, we can cleverly load the tensor onto the **CPU** but give this tensor a property belonging to **zynq**. Then, we check in the operator backend whether the tensor property list contains the **zynq** property. As long as both tensors involved in the operation have the **zynq** device property, the operation will be bound to the hardware accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a7aa2",
   "metadata": {},
   "source": [
    "## Demo 1: General Matrix Multiplication (GeMM) Operator Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35760ada",
   "metadata": {},
   "source": [
    "In this test, we directly call the `mmult` acceleration operation implemented in the operator. We built the above general matrix multiplication operator relying on a dual-pipeline (**pipeline**) **INT8** type **256**-dimension matrix multiplication accelerator. Compared to calling the **GeMM**-like algorithm executed by the **CPU**, the hardware **GeMM** we built has an extremely high speedup ratio. The test function definition is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db1df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_cpu(a, b, iters=3):\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    with torch.no_grad():\n",
    "        _ = torch.matmul(a, b)\n",
    "        t0 = time.perf_counter()\n",
    "        out = None\n",
    "        for _ in range(iters):\n",
    "            out = torch.matmul(a, b)\n",
    "        t1 = time.perf_counter()\n",
    "    return (t1 - t0) / iters, out\n",
    "\n",
    "def benchmark_fpga(a, b, iters=3):\n",
    "    with torch.no_grad():\n",
    "        _ = mmult(a, b)\n",
    "        t0 = time.perf_counter()\n",
    "        out = None\n",
    "        for _ in range(iters):\n",
    "            out = mmult(a, b)\n",
    "        t1 = time.perf_counter()\n",
    "    return (t1 - t0) / iters, out\n",
    "\n",
    "def make_inputs(n, m, p, kind):\n",
    "    if kind == \"FP32\":\n",
    "        a = torch.randn((n, m), dtype=torch.float32)\n",
    "        b = torch.randn((m, p), dtype=torch.float32)\n",
    "    elif kind == \"FP16\":\n",
    "        a = torch.randn((n, m), dtype=torch.float16)\n",
    "        b = torch.randn((m, p), dtype=torch.float16)\n",
    "    elif kind == \"INT8\":\n",
    "        a = torch.randint(-128, 127, (n, m), dtype=torch.int32)\n",
    "        b = torch.randint(-128, 127, (m, p), dtype=torch.int32)\n",
    "    else:\n",
    "        a = torch.randint(-128, 127, (n, m), dtype=torch.int32)\n",
    "        b = torch.randint(-128, 127, (m, p), dtype=torch.int32)\n",
    "    return a, b\n",
    "\n",
    "def test1():\n",
    "    n = 1024\n",
    "    m = 1024\n",
    "    p = 1024\n",
    "    iters = 1\n",
    "\n",
    "    ok = init()\n",
    "    kinds = [\"FP32\", \"FP16\", \"INT8\"]\n",
    "    print(\"\\n==================== MMULT BENCH ====================\")\n",
    "    print(f\"Size: ({n} x {m}) @ ({m} x {p})\")\n",
    "    for kind in kinds:\n",
    "        a, b = make_inputs(n, m, p, kind)\n",
    "        a_cpu = a.to(torch.int32) if a.dtype in (torch.float32, torch.float16, torch.int16) else a\n",
    "        b_cpu = b.to(torch.int32) if b.dtype in (torch.float32, torch.float16, torch.int16) else b\n",
    "        cpu_time, cpu_out = benchmark_cpu(a_cpu, b_cpu, iters)\n",
    "        fpga_time, fpga_out = benchmark_fpga(a_cpu, b_cpu, iters)\n",
    "        diff = (cpu_out - fpga_out).abs()\n",
    "        l2_err = torch.norm(cpu_out.float() - fpga_out.float()).item()\n",
    "        max_abs = diff.max().item()\n",
    "        eq_ratio = (diff == 0).float().mean().item()\n",
    "        speedup = cpu_time / fpga_time if fpga_time > 0 else float('inf')\n",
    "        print(f\"\\n[{kind}] CPU avg:   {cpu_time:.6f} s\")\n",
    "        print(f\"[{kind}] ZYNQ avg:  {fpga_time:.6f} s\")\n",
    "        print(f\"[{kind}] Speedup:   {speedup:.2f}x\")\n",
    "        print(f\"[{kind}] L2 error:  {l2_err:.6f}\")\n",
    "        print(f\"[{kind}] Max diff:  {max_abs}\")\n",
    "        print(f\"[{kind}] Exact %:   {eq_ratio*100:.2f}%\")\n",
    "    print(\"====================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77479b09",
   "metadata": {},
   "source": [
    "Now start the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb4c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a45a88",
   "metadata": {},
   "source": [
    "## Demo 2: CNN Forward Inference Benchmark based on Conv2D and F.linear Operations Accelerated by GeMM Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a1d74",
   "metadata": {},
   "source": [
    "On the PyTorch platform, we can deploy many frontend applications. Here we chose the famous **SpeechBrain** as the frontend toolkit to run the **ASR CN AIShell Speech Recognition** model. **SpeechBrain** has a user-friendly model deployment method. In order to fully utilize the hardware acceleration effect, we use **PyTorch**'s sub-toolkit `qnnpack` to perform dynamic quantization on the model, and use the `int8` data format to modify the weight layers except for `clc_in`. This allows us to evaluate the performance of our accelerator while retaining high precision.\n",
    "\n",
    "Below is the test framework code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e38a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import speechbrain\n",
    "from speechbrain.inference.ASR import EncoderDecoderASR\n",
    "from pytorch_zynq import (\n",
    "    register_zynq_device,\n",
    "    init_hardware as init,\n",
    "    deinit_hardware,\n",
    "    is_hardware_available,\n",
    "    enable_full_device,\n",
    "    disable_full_device,\n",
    ")\n",
    "\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "os.environ[\"TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\"] = \"yes\"\n",
    "\n",
    "def _apply_dynamic_quant(m):\n",
    "    targets = {\n",
    "        torch.nn.Linear,\n",
    "        torch.nn.LSTM,\n",
    "        torch.nn.GRU,\n",
    "        torch.nn.RNNCell,\n",
    "        torch.nn.GRUCell,\n",
    "        torch.nn.LSTMCell,\n",
    "        torch.nn.Embedding,\n",
    "        torch.nn.EmbeddingBag,\n",
    "    }\n",
    "    try:\n",
    "        torch.quantization.quantize_dynamic(\n",
    "            m.mods.encoder.transformer_encoder.transformer,\n",
    "            targets,\n",
    "            dtype=torch.qint8,\n",
    "            inplace=True,\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if hasattr(m.mods.encoder, \"enc\"):\n",
    "            torch.quantization.quantize_dynamic(\n",
    "                m.mods.encoder.enc,\n",
    "                targets,\n",
    "                dtype=torch.qint8,\n",
    "                inplace=True,\n",
    "            )\n",
    "    except Exception:\n",
    "        pass\n",
    "    return m\n",
    "\n",
    "\n",
    "def run_asr(source, device_str, wav, hparams_file, do_quant):\n",
    "    if device_str in (\"zynq\", \"privateuseone\"):\n",
    "        enable_full_device()\n",
    "    else:\n",
    "        disable_full_device()\n",
    "    m = EncoderDecoderASR.from_hparams(\n",
    "        source=source,\n",
    "        savedir=source,\n",
    "        run_opts={\"device\": device_str},\n",
    "        hparams_file=hparams_file,\n",
    "    )\n",
    "    try:\n",
    "        engines = getattr(torch.backends.quantized, \"supported_engines\", [])\n",
    "        if do_quant and (\"qnnpack\" in engines or \"fbgemm\" in engines) and device_str in (\"zynq\", \"privateuseone\"):\n",
    "            _apply_dynamic_quant(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "    with torch.no_grad():\n",
    "        t0 = time.perf_counter()\n",
    "        out = m.transcribe_file(wav)\n",
    "        t1 = time.perf_counter()\n",
    "    return out, t1 - t0\n",
    "\n",
    "\n",
    "def test2():\n",
    "    source = \"./ASR_CN\"\n",
    "    wav = \"./test2.wav\"\n",
    "    hparams = \"\"\n",
    "\n",
    "    register_zynq_device()\n",
    "    init()\n",
    "    hw = is_hardware_available()\n",
    "    print(f\"Hardware available: {hw}\")\n",
    "\n",
    "    # resolve hparams path: SpeechBrain joins savedir+filename internally,\n",
    "    # so pass only the filename and let source be the directory\n",
    "    if hparams:\n",
    "        if os.path.isabs(args.hparams) or os.path.sep in args.hparams:\n",
    "            args.source = os.path.dirname(args.hparams)\n",
    "            hparams_file = os.path.basename(args.hparams)\n",
    "        else:\n",
    "            hparams_file = hparams\n",
    "    else:\n",
    "        hparams_file = \"hyperparams.yaml\"\n",
    "    print(f\"Using source: {source}\")\n",
    "    print(f\"Using hparams: {hparams}\")\n",
    "\n",
    "    try:\n",
    "        engines = getattr(torch.backends.quantized, \"supported_engines\", [])\n",
    "        if args.quantized and (\"qnnpack\" not in engines and \"fbgemm\" not in engines):\n",
    "            print(\"[WARN] No quantized engine available on this platform; falling back to float model\")\n",
    "            hparams_file = \"hyperparams.yaml\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    text_cpu, t_cpu = run_asr(source, \"cpu\", wav, hparams_file, True)\n",
    "    text_fpga, t_fpga = (\"\", 0.0)\n",
    "    if hw:\n",
    "        text_fpga, t_fpga = run_asr(source, \"zynq\", wav, hparams_file, True)\n",
    "\n",
    "    print(\"\\n==================== ASR RESULTS ====================\")\n",
    "    print(f\"CPU time:  {t_cpu:.6f}s\")\n",
    "    if hw:\n",
    "        print(f\"ZYNQ time: {t_fpga:.6f}s\")\n",
    "        print(f\"Speedup:   {t_cpu / t_fpga:.2f}x\" if t_fpga > 0 else \"Speedup:   inf\")\n",
    "    print(f\"CPU text:  {text_cpu}\")\n",
    "    if hw:\n",
    "        print(f\"ZYNQ text: {text_fpga}\")\n",
    "    print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9ecd3",
   "metadata": {},
   "source": [
    "Due to limited on-board computing power, the benchmark test takes about **5 minutes**. Please execute the test as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7904db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}