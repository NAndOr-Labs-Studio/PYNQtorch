{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9567b36",
   "metadata": {},
   "source": [
    "# PYNQTorch 项目演示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bda187",
   "metadata": {},
   "source": [
    "先导入依赖和我们自定义的算子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aef69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "from pytorch_zynq import init_hardware as init, mmult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28365203",
   "metadata": {},
   "source": [
    "**AI加速**的基础是矩阵乘法，在torch后端注册`PrivateUseOne`接口并重命名到`zynq`可以很方便地注册我们想要的功能。我们需要在算子中声明设备的功能和行为，以下是注册的设备的声明：\n",
    "```python\n",
    "from .device import register_zynq_device, is_registered\n",
    "from .device import enable_full_device, disable_full_device\n",
    "from .device import enable_implicit_accel, disable_implicit_accel\n",
    "from .ops import mmult, register_aten_impls\n",
    "from .linear import ZynqLinear\n",
    "from .hardware import init as init_hardware, is_hardware_available, deinit as deinit_hardware\n",
    "\n",
    "__all__ = [\n",
    "    \"register_zynq_device\",\n",
    "    \"is_registered\",\n",
    "    \"mmult\",\n",
    "    \"ZynqLinear\",\n",
    "    \"init_hardware\",\n",
    "    \"is_hardware_available\",\n",
    "    \"register_aten_impls\",\n",
    "    \"deinit_hardware\",\n",
    "    \"enable_full_device\",\n",
    "    \"disable_full_device\",\n",
    "    \"enable_implicit_accel\",\n",
    "    \"disable_implicit_accel\",\n",
    "    \n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9c2c8",
   "metadata": {},
   "source": [
    "上面的代码展示了我们的双管线矩阵乘法（**GeMM**）算子支持的行为和加速操作。如何把张量绑定至算子呢？其实我们的硬件是不支持直接加载张量的，但是我们可以巧妙地将张量加载到**CPU**上，但是给这个张量一个属于**zynq**的属性，然后在算子后端检测张量属性列表是否含有**zynq**的属性，只要进行运算的两个张量都具有**zynq**的设备属性就将运算绑定到硬件加速器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a7aa2",
   "metadata": {},
   "source": [
    "## 演示1：通用矩阵乘（GeMM）算子基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35760ada",
   "metadata": {},
   "source": [
    "在这段测试中我们直接调用了算子中的实现的`mmult`加速操作。我们依托双管线（*pipeline*）**INT8**类型**256**维度矩阵乘法加速器构建了上述的通用矩阵乘算子，相较于调用**CPU**执行的类**GeMM**算法，我们构建的硬件**GeMM**具有极高的加速比，以下是测试函数定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db1df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_cpu(a, b, iters=3):\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    with torch.no_grad():\n",
    "        _ = torch.matmul(a, b)\n",
    "        t0 = time.perf_counter()\n",
    "        out = None\n",
    "        for _ in range(iters):\n",
    "            out = torch.matmul(a, b)\n",
    "        t1 = time.perf_counter()\n",
    "    return (t1 - t0) / iters, out\n",
    "\n",
    "def benchmark_fpga(a, b, iters=3):\n",
    "    with torch.no_grad():\n",
    "        _ = mmult(a, b)\n",
    "        t0 = time.perf_counter()\n",
    "        out = None\n",
    "        for _ in range(iters):\n",
    "            out = mmult(a, b)\n",
    "        t1 = time.perf_counter()\n",
    "    return (t1 - t0) / iters, out\n",
    "\n",
    "def make_inputs(n, m, p, kind):\n",
    "    if kind == \"FP32\":\n",
    "        a = torch.randn((n, m), dtype=torch.float32)\n",
    "        b = torch.randn((m, p), dtype=torch.float32)\n",
    "    elif kind == \"FP16\":\n",
    "        a = torch.randn((n, m), dtype=torch.float16)\n",
    "        b = torch.randn((m, p), dtype=torch.float16)\n",
    "    elif kind == \"INT8\":\n",
    "        a = torch.randint(-128, 127, (n, m), dtype=torch.int32)\n",
    "        b = torch.randint(-128, 127, (m, p), dtype=torch.int32)\n",
    "    else:\n",
    "        a = torch.randint(-128, 127, (n, m), dtype=torch.int32)\n",
    "        b = torch.randint(-128, 127, (m, p), dtype=torch.int32)\n",
    "    return a, b\n",
    "\n",
    "def test1():\n",
    "    n = 1024\n",
    "    m = 1024\n",
    "    p = 1024\n",
    "    iters = 1\n",
    "\n",
    "    ok = init()\n",
    "    kinds = [\"FP32\", \"FP16\", \"INT8\"]\n",
    "    print(\"\\n==================== MMULT BENCH ====================\")\n",
    "    print(f\"Size: ({n} x {m}) @ ({m} x {p})\")\n",
    "    for kind in kinds:\n",
    "        a, b = make_inputs(n, m, p, kind)\n",
    "        a_cpu = a.to(torch.int32) if a.dtype in (torch.float32, torch.float16, torch.int16) else a\n",
    "        b_cpu = b.to(torch.int32) if b.dtype in (torch.float32, torch.float16, torch.int16) else b\n",
    "        cpu_time, cpu_out = benchmark_cpu(a_cpu, b_cpu, iters)\n",
    "        fpga_time, fpga_out = benchmark_fpga(a_cpu, b_cpu, iters)\n",
    "        diff = (cpu_out - fpga_out).abs()\n",
    "        l2_err = torch.norm(cpu_out.float() - fpga_out.float()).item()\n",
    "        max_abs = diff.max().item()\n",
    "        eq_ratio = (diff == 0).float().mean().item()\n",
    "        speedup = cpu_time / fpga_time if fpga_time > 0 else float('inf')\n",
    "        print(f\"\\n[{kind}] CPU avg:   {cpu_time:.6f} s\")\n",
    "        print(f\"[{kind}] ZYNQ avg:  {fpga_time:.6f} s\")\n",
    "        print(f\"[{kind}] Speedup:   {speedup:.2f}x\")\n",
    "        print(f\"[{kind}] L2 error:  {l2_err:.6f}\")\n",
    "        print(f\"[{kind}] Max diff:  {max_abs}\")\n",
    "        print(f\"[{kind}] Exact %:   {eq_ratio*100:.2f}%\")\n",
    "    print(\"====================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77479b09",
   "metadata": {},
   "source": [
    "现在开始测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb4c736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\ntry {\nrequire(['notebook/js/codecell'], function(codecell) {\n  codecell.CodeCell.options_default.highlight_modes[\n      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n      Jupyter.notebook.get_cells().map(function(cell){\n          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n  });\n});\n} catch (e) {};\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\ntry {\nrequire(['notebook/js/codecell'], function(codecell) {\n  codecell.CodeCell.options_default.highlight_modes[\n      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n      Jupyter.notebook.get_cells().map(function(cell){\n          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n  });\n});\n} catch (e) {};\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MMULT BENCH ====================\n",
      "Size: (1024 x 1024) @ (1024 x 1024)\n",
      "\n",
      "[FP32] CPU avg:   17.803478 s\n",
      "[FP32] ZYNQ avg:  0.846731 s\n",
      "[FP32] Speedup:   21.03x\n",
      "[FP32] L2 error:  0.000000\n",
      "[FP32] Max diff:  0\n",
      "[FP32] Exact %:   100.00%\n",
      "\n",
      "[FP16] CPU avg:   20.213671 s\n",
      "[FP16] ZYNQ avg:  0.843857 s\n",
      "[FP16] Speedup:   23.95x\n",
      "[FP16] L2 error:  0.000000\n",
      "[FP16] Max diff:  0\n",
      "[FP16] Exact %:   100.00%\n",
      "\n",
      "[INT8] CPU avg:   20.214558 s\n",
      "[INT8] ZYNQ avg:  0.847224 s\n",
      "[INT8] Speedup:   23.86x\n",
      "[INT8] L2 error:  0.000000\n",
      "[INT8] Max diff:  0\n",
      "[INT8] Exact %:   100.00%\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "test1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a45a88",
   "metadata": {},
   "source": [
    "## 演示2：基于GeMM算子加速的Conv2D与F.linear运算操作进行CNN前向推理基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a1d74",
   "metadata": {},
   "source": [
    "在PyTorch平台上，我们可以部署许多的前端应用。这里我们选择了著名的**SpeechBrain**作为前端工具集，运行**ASR CN AIShell语音识别**模型。**SpeechBrain**有着用户友好的模型部署方式。为了充分发挥硬件加速效果，我们使用**PyTorch**的子工具集`qnnpack`对模型进行动态量化，使用`int8`数据格式对除了`clc_in`外的权重层进行修改，在保留较高精度的同时可以评估我们加速器的性能。\n",
    "\n",
    "以下是测试框架代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e38a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/pynq-venv/lib/python3.10/site-packages/speechbrain/utils/torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  available_backends = torchaudio.list_audio_backends()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import speechbrain\n",
    "from speechbrain.inference.ASR import EncoderDecoderASR\n",
    "from pytorch_zynq import (\n",
    "    register_zynq_device,\n",
    "    init_hardware as init,\n",
    "    deinit_hardware,\n",
    "    is_hardware_available,\n",
    "    enable_full_device,\n",
    "    disable_full_device,\n",
    ")\n",
    "\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "os.environ[\"TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\"] = \"yes\"\n",
    "\n",
    "def _apply_dynamic_quant(m):\n",
    "    targets = {\n",
    "        torch.nn.Linear,\n",
    "        torch.nn.LSTM,\n",
    "        torch.nn.GRU,\n",
    "        torch.nn.RNNCell,\n",
    "        torch.nn.GRUCell,\n",
    "        torch.nn.LSTMCell,\n",
    "        torch.nn.Embedding,\n",
    "        torch.nn.EmbeddingBag,\n",
    "    }\n",
    "    try:\n",
    "        torch.quantization.quantize_dynamic(\n",
    "            m.mods.encoder.transformer_encoder.transformer,\n",
    "            targets,\n",
    "            dtype=torch.qint8,\n",
    "            inplace=True,\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if hasattr(m.mods.encoder, \"enc\"):\n",
    "            torch.quantization.quantize_dynamic(\n",
    "                m.mods.encoder.enc,\n",
    "                targets,\n",
    "                dtype=torch.qint8,\n",
    "                inplace=True,\n",
    "            )\n",
    "    except Exception:\n",
    "        pass\n",
    "    return m\n",
    "\n",
    "\n",
    "def run_asr(source, device_str, wav, hparams_file, do_quant):\n",
    "    if device_str in (\"zynq\", \"privateuseone\"):\n",
    "        enable_full_device()\n",
    "    else:\n",
    "        disable_full_device()\n",
    "    m = EncoderDecoderASR.from_hparams(\n",
    "        source=source,\n",
    "        savedir=source,\n",
    "        run_opts={\"device\": device_str},\n",
    "        hparams_file=hparams_file,\n",
    "    )\n",
    "    try:\n",
    "        engines = getattr(torch.backends.quantized, \"supported_engines\", [])\n",
    "        if do_quant and (\"qnnpack\" in engines or \"fbgemm\" in engines) and device_str in (\"zynq\", \"privateuseone\"):\n",
    "            _apply_dynamic_quant(m)\n",
    "    except Exception:\n",
    "        pass\n",
    "    with torch.no_grad():\n",
    "        t0 = time.perf_counter()\n",
    "        out = m.transcribe_file(wav)\n",
    "        t1 = time.perf_counter()\n",
    "    return out, t1 - t0\n",
    "\n",
    "\n",
    "def test2():\n",
    "    source = \"./ASR_CN\"\n",
    "    wav = \"./test2.wav\"\n",
    "    hparams = \"\"\n",
    "\n",
    "    register_zynq_device()\n",
    "    init()\n",
    "    hw = is_hardware_available()\n",
    "    print(f\"Hardware available: {hw}\")\n",
    "\n",
    "    # resolve hparams path: SpeechBrain joins savedir+filename internally,\n",
    "    # so pass only the filename and let source be the directory\n",
    "    if hparams:\n",
    "        if os.path.isabs(args.hparams) or os.path.sep in args.hparams:\n",
    "            args.source = os.path.dirname(args.hparams)\n",
    "            hparams_file = os.path.basename(args.hparams)\n",
    "        else:\n",
    "            hparams_file = hparams\n",
    "    else:\n",
    "        hparams_file = \"hyperparams.yaml\"\n",
    "    print(f\"Using source: {source}\")\n",
    "    print(f\"Using hparams: {hparams}\")\n",
    "\n",
    "    try:\n",
    "        engines = getattr(torch.backends.quantized, \"supported_engines\", [])\n",
    "        if args.quantized and (\"qnnpack\" not in engines and \"fbgemm\" not in engines):\n",
    "            print(\"[WARN] No quantized engine available on this platform; falling back to float model\")\n",
    "            hparams_file = \"hyperparams.yaml\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    text_cpu, t_cpu = run_asr(source, \"cpu\", wav, hparams_file, True)\n",
    "    text_fpga, t_fpga = (\"\", 0.0)\n",
    "    if hw:\n",
    "        text_fpga, t_fpga = run_asr(source, \"zynq\", wav, hparams_file, True)\n",
    "\n",
    "    print(\"\\n==================== ASR RESULTS ====================\")\n",
    "    print(f\"CPU time:  {t_cpu:.6f}s\")\n",
    "    if hw:\n",
    "        print(f\"ZYNQ time: {t_fpga:.6f}s\")\n",
    "        print(f\"Speedup:   {t_cpu / t_fpga:.2f}x\" if t_fpga > 0 else \"Speedup:   inf\")\n",
    "    print(f\"CPU text:  {text_cpu}\")\n",
    "    if hw:\n",
    "        print(f\"ZYNQ text: {text_fpga}\")\n",
    "    print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9ecd3",
   "metadata": {},
   "source": [
    "由于板端算力有限，执行基准测试大约需要**5分钟**，请视情况执行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7904db84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware available: True\n",
      "Using source: ./ASR_CN\n",
      "Using hparams: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/pynq-venv/lib/python3.10/site-packages/speechbrain/processing/features.py:1529: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "/usr/local/share/pynq-venv/lib/python3.10/site-packages/speechbrain/utils/checkpoints.py:200: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/usr/local/share/pynq-venv/lib/python3.10/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1687/3409608602.py:28: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== ASR RESULTS ====================\n",
      "CPU time:  294.368227s\n",
      "ZYNQ time: 227.967185s\n",
      "Speedup:   1.29x\n",
      "CPU text:  嵌入 式 高层 次 综合 赛道\n",
      "ZYNQ text: 嵌入 式 高层 次 综合 赛道\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "test2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
